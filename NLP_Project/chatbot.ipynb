{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, json, pdfplumber\n",
    "import tkinter as tk\n",
    "from transformers import pipeline\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import ConversationStringBufferMemory\n",
    "from langchain.schema import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain.prompts import PromptTemplate\n",
    "from guardrails import Guard\n",
    "from guardrails.hub import NSFWText, ToxicLanguage\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import SQLChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--facebook--bart-large-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "with open(\"classifier_data.json\", \"r\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "labels = data[\"labels\"]\n",
    "nlp_keywords = data[\"nlp_keywords\"]\n",
    "folder_path = \"Courses\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_with_pdfplumber(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file using pdfplumber.\"\"\"\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def extract_documents_from_folder(folder_path):\n",
    "    \"\"\"Extract text from all PDF files in a folder and return as a list of Documents.\"\"\"\n",
    "    documents = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            print(f\"Extracting text from {filename}...\")\n",
    "            text = extract_text_with_pdfplumber(file_path)\n",
    "\n",
    "            document = Document(page_content=text, metadata={\"source\": filename})\n",
    "            documents.append(document)\n",
    "    return documents\n",
    "\n",
    "def split_text_into_chunks(documents, chunk_size=700, chunk_overlap=50):\n",
    "    \"\"\"Split documents into smaller chunks for embedding.\"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = splitter.split_documents(documents)\n",
    "    return chunks\n",
    "\n",
    "def create_faiss_index(chunks, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    \"\"\"Create a FAISS vector index from text chunks using Sentence Transformers.\"\"\"\n",
    "    print(f\"Loading SentenceTransformer model: {model_name}...\")\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        encode_kwargs = {'normalize_embeddings': True} \n",
    "    )\n",
    "    faiss_index = FAISS.from_documents(chunks, embedding_model)\n",
    "\n",
    "    faiss_index.save_local(\"faiss_index\")\n",
    "    return faiss_index\n",
    "\n",
    "def load_faiss_index(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "    print(f\"Loading SentenceTransformer model: {model_name}...\")\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        encode_kwargs = {'normalize_embeddings': True} \n",
    "    )\n",
    "\n",
    "    faiss_index = FAISS.load_local(\"faiss_index\", embedding_model, allow_dangerous_deserialization=True)\n",
    "    return faiss_index\n",
    "\n",
    "def generate_faiss_index(folder_path):\n",
    "    \n",
    "    print(\"Starting text extraction...\")\n",
    "    documents = extract_documents_from_folder(folder_path)\n",
    "    \n",
    "    print(\"Splitting text into chunks...\")\n",
    "    chunks = split_text_into_chunks(documents)\n",
    "    \n",
    "    print(\"Creating FAISS index...\")\n",
    "    return create_faiss_index(chunks, model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def format_docs(docs): \n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def get_prompt_template():\n",
    "    template = \"\"\"\n",
    "        Using only the given context about Natural Language Processing and Large Language Models, answer the user's question.\n",
    "        Please follow the following rules:\n",
    "            1. If you don't know the answer, don't try to make up an answer. Just say \"I can't find the final answer.\"\n",
    "            2. If you find the answer, write the answer concisely with at most two sentences.\n",
    "\n",
    "        {context}\n",
    "        Conversation history:\n",
    "        {chat_history}\n",
    "\n",
    "        Query: {question}\n",
    "        \"\"\"\n",
    "    return PromptTemplate(template=template, input_variables=[\"context\", \"question\", \"chat_history\"])\n",
    "    \n",
    "def custom_on_fail(value, fail_result):\n",
    "    return \"Please retain from using profanity with our model\"\n",
    "    \n",
    "def get_session_history(session_id):\n",
    "    return SQLChatMessageHistory(session_id, \"sqlite:///memory.db\")\n",
    "\n",
    "def setup_retrieval_qa(faiss_index, threshold=0.5):\n",
    "    \n",
    "    llm = HuggingFaceEndpoint(\n",
    "        repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "        temperature= 0.5\n",
    "    )\n",
    "    retriever = faiss_index.as_retriever(\n",
    "        score_threshold=threshold,\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\": 5},  \n",
    "    )\n",
    "\n",
    "    guard = Guard().use_many(\n",
    "        ToxicLanguage(threshold=0.95, validation_method=\"sentence\", on_fail=custom_on_fail),\n",
    "        NSFWText(threshold=0.95, validation_method=\"sentence\", on_fail=custom_on_fail)\n",
    "        )\n",
    "\n",
    "    memory = ConversationStringBufferMemory(memory_key=\"chat_history\", return_messages=False)\n",
    "    prompt_template = get_prompt_template()\n",
    "    runnable = prompt_template | llm | guard.to_runnable()\n",
    "\n",
    "    runnable_with_history = RunnableWithMessageHistory(\n",
    "        runnable,\n",
    "        get_session_history,\n",
    "        input_messages_key=\"question\",\n",
    "        history_messages_key=\"chat_history\",\n",
    "    )\n",
    "    return retriever, runnable_with_history\n",
    "\n",
    "def classify_question(question: str):\n",
    "    question_lower = question.lower()\n",
    "    for keyword in nlp_keywords:\n",
    "        if keyword in question_lower:\n",
    "            return \"Natural Language Processing\", 1.0\n",
    "\n",
    "    result = classifier(question, labels)\n",
    "    label = result[\"labels\"][0]\n",
    "    score = result[\"scores\"][0] \n",
    "    return label, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting text extraction...\n",
      "Extracting text from 0. Course Introduction.pdf...\n",
      "Extracting text from 1. NLP Overview.pdf...\n",
      "Extracting text from 10. Transformers II.pdf...\n",
      "Extracting text from 11. From Transformers to LLMs.pdf...\n",
      "Extracting text from 12. HuggingFace.pdf...\n",
      "Extracting text from 13. Encoder-only Transformers.pdf...\n",
      "Extracting text from 14. Decoder-only Transformers.pdf...\n",
      "Extracting text from 15. Encoder-Decoder Transformers.pdf...\n",
      "Extracting text from 17. Fine tuning.pdf...\n",
      "Extracting text from 18. Prompt Engineering.pdf...\n",
      "Extracting text from 2. Representing Text.pdf...\n",
      "Extracting text from 20. RAG.pdf...\n",
      "Extracting text from 21. RLHF for LLMs.pdf...\n",
      "Extracting text from 22. Guardrails for LLMs.pdf...\n",
      "Extracting text from 3. Math with Words.pdf...\n",
      "Extracting text from 4. Text Classification.pdf...\n",
      "Extracting text from 5. Word Embeddings.pdf...\n",
      "Extracting text from 6. Neural Networks for NLP.pdf...\n",
      "Extracting text from 7. Dialog Engines.pdf...\n",
      "Extracting text from 8. Building a Chatbot.pdf...\n",
      "Extracting text from 9. Transformers I.pdf...\n",
      "Splitting text into chunks...\n",
      "Creating FAISS index...\n",
      "Loading SentenceTransformer model: sentence-transformers/all-MiniLM-L6-v2...\n",
      "Setting up the chatbot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_5896\\1543097528.py:105: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationStringBufferMemory(memory_key=\"chat_history\", return_messages=False)\n"
     ]
    }
   ],
   "source": [
    "#for first time run create the index\n",
    "#faiss_index = generate_faiss_index(folder_path)\n",
    "    \n",
    "#for next run just load the created index\n",
    "faiss_index = load_faiss_index(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    \n",
    "print(\"Setting up the chatbot...\")\n",
    "retriever, my_rag_chain = setup_retrieval_qa(faiss_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(user_input):\n",
    "\n",
    "    if not user_input or len(user_input) == 0 or user_input == \"\":\n",
    "        return \"Please enter a question.\"\n",
    "    \n",
    "    print(f'Question: {user_input}')\n",
    "    \n",
    "    label, score = classify_question(user_input)\n",
    "\n",
    "    if label != \"Natural Language Processing\" or score < 0.55:\n",
    "        return \"I'm only able to answer questions about NLP, LLMs, and the related course materials.\"\n",
    "    \n",
    "    matches = retriever.invoke(user_input) \n",
    "    relevant_chunks = \" \".join([match.page_content for match in matches])\n",
    "\n",
    "    try:\n",
    "        result = my_rag_chain.invoke(\n",
    "            {\"context\": relevant_chunks, \"question\": user_input},\n",
    "            {\"configurable\": {\"session_id\": \"4\"}})\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return \"There was an error.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2107824970048submit_query'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def submit_query(event=None):\n",
    "    user_input = entry.get()\n",
    "    if user_input.lower() == 'exit':\n",
    "        chat_box.insert(tk.END, \"Chatbot: Goodbye!\\n\\n\")\n",
    "        chat_box.yview(tk.END)\n",
    "        window.quit()\n",
    "        window.destroy() \n",
    "        return\n",
    "    \n",
    "    response = chatbot(user_input)\n",
    "    print(response)\n",
    "    \n",
    "    chat_box.config(state=tk.NORMAL)\n",
    "    chat_box.insert(tk.END, f\"User: {user_input}\\nChatbot: {response}\\n\\n\")\n",
    "    chat_box.yview(tk.END)\n",
    "    chat_box.config(state=tk.DISABLED)\n",
    "    \n",
    "    entry.delete(0, tk.END)\n",
    "    \n",
    "window = tk.Tk()\n",
    "window.title(\"Chatbot UI\")\n",
    "window.geometry(\"800x500\")\n",
    "\n",
    "chat_box = tk.Text(window, height=25, width=80, state=tk.DISABLED, wrap=tk.WORD)\n",
    "chat_box.pack(pady=10)\n",
    "\n",
    "entry = tk.Entry(window, width=80)\n",
    "entry.pack(pady=5)\n",
    "\n",
    "entry.bind(\"<Return>\", submit_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is LLM?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\runnables\\history.py:608: LangChainDeprecationWarning: `connection_string` was deprecated in LangChain 0.2.2 and will be removed in 1.0. Use connection instead.\n",
      "  message_history = self.get_session_history(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\guardrails\\validator_service\\__init__.py:85: UserWarning: Could not obtain an event loop. Falling back to synchronous validation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Response: LLM, or Large Language Model, is an advanced model used in Natural Language Processing (NLP) for understanding and generating human-like text. It is trained on extensive data and computational power to understand and generate text based on context and meaning. LLMs are used in various applications such as text generation, machine translation, chatbots, code generation, question answering, text summarization, writing assistance, and multimodal LLM. They have been proven to be effective in these applications and have significantly advanced the field of NLP. For example, they have improved the quality of machine translation, enabled the development of advanced chatbots, and facilitated code generation and summarization tasks. LLMs are a valuable tool for researchers, developers, and businesses in the field of NLP, and their usefulness continues to grow as new applications and use cases are discovered.\n",
      "Question: Are you sure?\n",
      "I'm only able to answer questions about NLP, LLMs, and the related course materials.\n",
      "Question: Are you sure about LLM?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\guardrails\\validator_service\\__init__.py:85: UserWarning: Could not obtain an event loop. Falling back to synchronous validation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Response: Yes, I'm an assistant specifically designed to help answer questions related to Natural Language Processing, Large Language Models, and the provided course materials. I can provide accurate and detailed information about the concepts, applications, and technologies related to these topics. I'm here to help clarify any doubts or questions you may have, so feel free to ask me anything related to these topics. I cannot answer questions about personal names or other unrelated topics.\n",
      "\n",
      "        LLM, or Large Language Model, is an advanced model used in Natural Language Processing (NLP) for understanding and generating human-like text. It is trained on extensive data and computational power to understand and generate text based on context and meaning. LLMs are used in various applications such as text generation, machine translation, chatbots, code generation, question answering, text summarization, writing assistance, and multimodal LLM. They have been proven to be effective in these applications and have significantly advanced the field of NLP. For example, they have improved the quality of machine translation, enabled the development of advanced chatbots, and facilitated code generation and summarization tasks. LLMs are a valuable tool for researchers, developers, and businesses in the field of NLP, and their usefulness continues to grow as new applications and use cases are discovered.\n",
      "\n",
      "        The user asked if I'm sure about my abilities to answer questions related to NLP and LLMs. I assured the user that I'm an assistant specifically designed to help answer questions related to these topics and provided a detailed and informative answer explaining what LLMs are, their applications, and their significance in the field of NLP. I also emphasized that I cannot answer questions about personal names or other unrelated topics.\n",
      "Question: Who are the teachers of the course?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\guardrails\\validator_service\\__init__.py:85: UserWarning: Could not obtain an event loop. Falling back to synchronous validation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Response: Nicola Capuano and Antonio Greco are the teachers of the course.\n",
      "Question: And what about the exam?\n",
      "I'm only able to answer questions about NLP, LLMs, and the related course materials.\n",
      "Question: What do you know about the exam for LLM?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\guardrails\\validator_service\\__init__.py:85: UserWarning: Could not obtain an event loop. Falling back to synchronous validation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Response: I'm an assistant specifically designed to help answer questions related to Natural Language Processing, Large Language Models, and the provided course materials. However, I cannot find specific information about an exam for LLM in the context you've given. If you could provide more context or details about the exam, such as the name, date, or format, I would be happy to help you with that. In the meantime, if you have any questions related to LLMs, NLP, or the provided course materials, feel free to ask me. I cannot answer questions about personal names or other unrelated topics.\n",
      "Question: What about the exam for NLP?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\guardrails\\validator_service\\__init__.py:85: UserWarning: Could not obtain an event loop. Falling back to synchronous validation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Response: I'm an assistant specifically designed to help answer questions related to Natural Language Processing, Large Language Models, and the provided course materials. However, I cannot find specific information about an exam for NLP in the context you've given. If you could provide more context or details about the exam, such as the name, date, or format, I would be happy to help you with that. In the meantime, if you have any questions related to NLP, LLMs, or the provided course materials, feel free to ask me. I cannot answer questions about personal names or other unrelated topics.\n",
      "Question: What about the exam of the course?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\guardrails\\validator_service\\__init__.py:85: UserWarning: Could not obtain an event loop. Falling back to synchronous validation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Response: I'm an assistant specifically designed to help answer questions related to Natural Language Processing, Large Language Models, and the provided course materials. However, I cannot find specific information about an exam for the course in the context you've given. If you could provide more context or details about the exam, such as the name, date, or format, I would be happy to help you with that. In the meantime, if you have any questions related to NLP, LLMs, or the provided course materials, feel free to ask me. I cannot answer questions about personal names or other unrelated topics.\n",
      "Question: What about the teachers of the course?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\guardrails\\validator_service\\__init__.py:85: UserWarning: Could not obtain an event loop. Falling back to synchronous validation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Response: Nicola Capuano and Antonio Greco are the teachers of the course.\n",
      "Question: And what do you know about them?\n",
      "I'm only able to answer questions about NLP, LLMs, and the related course materials.\n",
      "Question: And what do you know about the teachers?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\guardrails\\validator_service\\__init__.py:85: UserWarning: Could not obtain an event loop. Falling back to synchronous validation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Response: Nicola Capuano and Antonio Greco are the teachers of the course.\n",
      "Question: How to contact the teachers?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\guardrails\\validator_service\\__init__.py:85: UserWarning: Could not obtain an event loop. Falling back to synchronous validation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Based on the given context, the teachers' contact information is not provided directly in the conversation history. However, it is mentioned in the example-classes tag that their names are Nicola Capuano and Antonio Greco. To contact them, one could look up their contact information using a search engine or the university's website.\n",
      "\n",
      "        Answer: To contact the teachers, Nicola Capuano and Antonio Greco, you can look up their contact information using a search engine or the university's website.\n",
      "Question: What is Bag of words?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\guardrails\\validator_service\\__init__.py:85: UserWarning: Could not obtain an event loop. Falling back to synchronous validation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Response: Bag of words is a technique for representing text data as numerical vectors. Each word is assigned an index that represents its position in the vocabulary, and the text is represented as a vector of the indices of the words it contains. The distance between any two vectors is always the same, but it does not capture the semantics of words and is not efficient due to sparse vectors. It is often used as a baseline method for text classification tasks.\n",
      "Question: What is a transformer?\n",
      "I'm only able to answer questions about NLP, LLMs, and the related course materials.\n",
      "Question: What are transformers?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\guardrails\\validator_service\\__init__.py:85: UserWarning: Could not obtain an event loop. Falling back to synchronous validation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Answer: Transformers are a type of deep learning model architecture introduced by Vaswani et al. in 2017 for handling sequential data, such as text or speech. They use self-attention mechanisms to allow each position in the sequence to attend to all other positions, enabling the model to focus on relevant information in the input sequence for each output token. This allows the model to capture long-range dependencies and context in the data, making it particularly effective for natural language processing tasks. Transformers have been widely adopted in various NLP applications, such as machine translation, question answering, and text generation.\n",
      "Question: What is tokenization?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\guardrails\\validator_service\\__init__.py:85: UserWarning: Could not obtain an event loop. Falling back to synchronous validation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Answer: Tokenization is the process of breaking down text into smaller units, called tokens, for analysis by a computer. In Natural Language Processing (NLP), tokens can be words, punctuation marks, or subwords, depending on the specific tokenization method used. Tokenization helps to simplify complex text data and make it easier for machines to understand and process. For instance, the WordPiece model uses subword tokenization to break down words into smaller meaningful components, while the bag-of-words model represents text as a vector of word indices.\n",
      "Question: What are RNNs?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\guardrails\\validator_service\\__init__.py:85: UserWarning: Could not obtain an event loop. Falling back to synchronous validation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        RNNs, or Recurrent Neural Networks, are a type of deep learning model architecture used for processing sequential data, such as text, speech, or time series data. Unlike traditional feedforward neural networks, RNNs have a recurrent connection that allows information from previous time steps to influence the current time step, enabling the model to capture temporal dependencies and context in the data. This makes RNNs particularly effective for natural language processing tasks, such as language translation, speech recognition, and text generation. RNNs can be unidirectional, processing the sequence only in one direction, or bidirectional, processing it in both directions, which can help capture more complex patterns and relationships in the data.\n",
      "Question: And what are their limits in NLP?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\guardrails\\validator_service\\__init__.py:85: UserWarning: Could not obtain an event loop. Falling back to synchronous validation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Response: The limits of Term Frequency (TF) include the inability to handle word variations and punctuation correctly. For instance, the word \"dog\" may appear more frequently in a short email to a veterinarian than in the novel \"War and Peace,\" but it is not necessarily more important in the context of the document. Normalized Term Frequency (TF-IDF) addresses this issue by considering both the frequency of a word in a document and its frequency in the entire corpus. However, it still has limitations in handling word variations and complexities, such as synonyms, negations, and idiomatic expressions. To handle these complexities, more advanced NLP techniques, such as Word Embeddings, are used.\n",
      "Question: What are different types of word embeddings?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\guardrails\\validator_service\\__init__.py:85: UserWarning: Could not obtain an event loop. Falling back to synchronous validation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Response: Word embeddings are numerical representations of words that capture their meaning and relationships with other words. There are two main types of word embeddings: Static and Contextual.\n",
      "\n",
      "        Static embeddings, such as Word2Vec, GloVe, and FastText, represent each word as a single static vector that captures the average meaning of the word based on the training corpus. These vectors do not change based on context and do not account for polysemy and homonymy.\n",
      "\n",
      "        Contextual embeddings, such as ELMo and BERT, can be updated based on the context of surrounding words. They are particularly effective for applications that need deep language understanding, such as machine translation and question answering. In contextual embeddings, the embedding for a word changes depending on the context, making them more accurate in capturing the meaning of words in various contexts.\n",
      "\n",
      "        For example, the word \"bank\" can refer to a financial institution or the side of a river. A static embedding would blend these meanings into a single vector, failing to capture the specific context. However, a contextual embedding would generate different vectors for the word \"bank\" depending on the context, accurately representing its meaning in each context.\n",
      "Question: How do transformers address the vanishing gradient problem?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\guardrails\\validator_service\\__init__.py:85: UserWarning: Could not obtain an event loop. Falling back to synchronous validation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Transformers use self-attention mechanisms, which allow each position in the sequence to attend to all other positions, enabling the model to focus on relevant information in the input sequence for each output token. This allows the model to capture long-range dependencies and context in the data, making it particularly effective for natural language processing tasks. Transformers have been shown to be more effective than RNNs in handling long-range dependencies and addressing the vanishing gradient problem.\n",
      "\n",
      "        Transformers' self-attention mechanisms enable the model to compute the weighted sum of input tokens based on their relevance to each other, which helps maintain the gradients during backpropagation. In contrast, RNNs suffer from the vanishing gradient problem, where the gradients become smaller and eventually disappear as they propagate through many time steps, making it difficult to learn long-term dependencies and patterns in the data. By using self-attention mechanisms, transformers can maintain the gradients and effectively learn long-term dependencies and context in the data, making them more effective for natural language processing tasks.\n",
      "Question: What is tf-idf?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\guardrails\\validator_service\\__init__.py:85: UserWarning: Could not obtain an event loop. Falling back to synchronous validation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical statistic used in information retrieval and text mining to reflect how important a word is to a document in a collection or corpus. It is based on the idea that the importance of a word to a document should be measured by both its frequency (TF) in the document and its rarity (IDF) in the corpus. TF-IDF is calculated as the product of the term frequency (TF) and the inverse document frequency (IDF) of a word in a document collection. It is commonly used in text search engines, text mining, and information retrieval systems to rank the importance of words and documents. TF-IDF matrices have been the mainstay of information retrieval (search) for decades. The process involves tokenizing all documents and creating a TF-IDF matrix, where each row represents a document and each column represents a term in the corpus. IDF gives more weightage to the words that are less frequent in the corpus, and high TF-IDF indicates a term that is frequent in a document but rare in the corpus, making it significant for that document. Low TF-IDF indicates a term that is infrequent in the document or common in the corpus, making it less significant for that document.\n",
      "Question: What is tf-idf and how it is calculated?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\guardrails\\validator_service\\__init__.py:85: UserWarning: Could not obtain an event loop. Falling back to synchronous validation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please retain from using profanity with our model\n",
      "Question: Describe the self attention method\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\guardrails\\validator_service\\__init__.py:85: UserWarning: Could not obtain an event loop. Falling back to synchronous validation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Response: The self-attention method is a mechanism used in Transformer models for processing sequential data, such as text or speech. It allows each position in the sequence to attend to all other positions, enabling the model to capture long-range dependencies and context in the data. In self-attention, the input sequence is split into three parts: queries (Q), keys (K), and values (V). Each position in the sequence computes a dot product between the queries and keys, and the results are scaled and passed through a softmax function to get the attention weights. The values are then weighted by the attention weights and summed to produce the output for each position. This process is repeated for each position in the sequence, resulting in a sequence of outputs that captures the relationships and dependencies between the positions in the input sequence. Self-attention enables the model to focus on relevant information in the input sequence for each output token, making it particularly effective for natural language processing tasks.\n",
      "Question: What is it's role in the transformers model?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\guardrails\\validator_service\\__init__.py:85: UserWarning: Could not obtain an event loop. Falling back to synchronous validation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        The self-attention mechanism is a core component of the Transformers model architecture. It allows each position in the sequence to attend to all other positions, enabling the model to capture long-range dependencies and context in the data. This is particularly useful for natural language processing tasks, where understanding the relationships and dependencies between words and phrases is essential for accurate meaning representation. By using self-attention, the Transformers model can effectively learn the relationships and dependencies between positions in the input sequence, making it more effective for natural language processing tasks than traditional recurrent neural networks (RNNs).\n",
      "Question: What is the textbook of the course?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\guardrails\\validator_service\\__init__.py:85: UserWarning: Could not obtain an event loop. Falling back to synchronous validation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Response: The textbooks for the course are BookCorpus and Gutenberg. These datasets include a wide range of literary genres and contribute to the models' pre-training by exposing them to a diverse array of textual genres and subject matter, fostering a more comprehensive understanding of language across various domains. Book Corpus includes 800 million words. Common Crawl manages an accessible repository of web outputs, which is also used for content filtering and fine-tuning purposes.\n",
      "Question: Who is one of the lectures of the course?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\guardrails\\validator_service\\__init__.py:85: UserWarning: Could not obtain an event loop. Falling back to synchronous validation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Response: Nicola Capuano is one of the lecturers of the course.\n",
      "Question: Are you sure?\n",
      "I'm only able to answer questions about NLP, LLMs, and the related course materials.\n",
      "Question: Are you sure he is one of the lecturers of the course?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\guardrails\\validator_service\\__init__.py:85: UserWarning: Could not obtain an event loop. Falling back to synchronous validation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Response: Yes, Nicola Capuano is one of the lecturers of the course, as stated in the provided context.\n",
      "Question: What is the unit of electrical voltage?\n",
      "I'm only able to answer questions about NLP, LLMs, and the related course materials.\n",
      "Question: Which is the program of the NLP course at University of Bologna?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\guardrails\\validator_service\\__init__.py:85: UserWarning: Could not obtain an event loop. Falling back to synchronous validation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        Response: I cannot directly answer that question as the context provided does not mention the specific NLP course or university you are referring to. However, I can provide some general information about NLP courses and the University of Bologna.\n",
      "\n",
      "        NLP courses typically cover topics such as natural language understanding, natural language generation, text mining, information retrieval, and machine translation. They may also include practical projects and assignments to help students gain hands-on experience in applying NLP techniques to real-world data.\n",
      "\n",
      "        The University of Bologna, also known as Alma Mater Studiorum - Università di Bologna, is one of the oldest universities in Europe, founded in 1088. It offers a wide range of degree programs, including several related to Computer Science and Engineering, which may include NLP courses. For more specific information about NLP courses offered at the University of Bologna, I would recommend checking their official website or contacting their department of Computer Science or Linguistics directly.\n"
     ]
    }
   ],
   "source": [
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
